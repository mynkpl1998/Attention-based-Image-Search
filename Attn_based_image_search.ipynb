{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0RzdMTsJYA2hK2oVz3cQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mynkpl1998/Attention-based-Image-Search/blob/main/Attn_based_image_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "qvI21ZcpAX_p"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torchvision.transforms import v2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision.transforms.functional as F\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from tabulate import tabulate\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from typing import Any, Callable, List, Optional, Tuple, Union"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SquarePad:\n",
        "\tdef __call__(self, image):\n",
        "\t\tw, h = image.size\n",
        "\t\tmax_wh = np.max([w, h])\n",
        "\t\thp = int((max_wh - w) / 2)\n",
        "\t\tvp = int((max_wh - h) / 2)\n",
        "\t\tpadding = (hp, vp, hp, vp)\n",
        "\t\treturn F.pad(image, padding, 0, 'constant')\n",
        "\n",
        "class Caltech5(Dataset):\n",
        "\n",
        "  def __init__(self,\n",
        "               split_ratio: float,\n",
        "               split: str,\n",
        "               transform: Optional[Callable] = None,\n",
        "               target_transform: Optional[Callable] = None):\n",
        "\n",
        "    use_labels = [\"motorbikes\", \"butterfly\", \"accordion\", \"airplanes\", \"brain\"]\n",
        "\n",
        "    assert split == \"train\" or split == \"test\"\n",
        "    self._split_type = split\n",
        "    assert len(use_labels) == 5\n",
        "    assert split_ratio > 0.0 and split_ratio <= 1.0\n",
        "\n",
        "    if transform is not None:\n",
        "      self._img_transform = transform\n",
        "    else:\n",
        "      self._img_transform = None\n",
        "\n",
        "    img_dims = (300, 300)\n",
        "    self._base_img_transforms = v2.Compose([\n",
        "        SquarePad(),\n",
        "        v2.Resize(img_dims)\n",
        "    ])\n",
        "\n",
        "    # Complete 101 dataset\n",
        "    dataloader = datasets.Caltech101(root=\"./\", download=True)\n",
        "\n",
        "    # Labels map\n",
        "    labels_map = {}\n",
        "    labels_map[3] = \"motorbikes\"\n",
        "    labels_map[4] = \"accordion\"\n",
        "    labels_map[5] = \"airplanes\"\n",
        "    labels_map[13] = \"brain\"\n",
        "    labels_map[16] = \"butterfly\"\n",
        "\n",
        "    self._labels_map = labels_map\n",
        "\n",
        "    # label text -> idx\n",
        "    reverse_label_map = { v:k for k,v in labels_map.items() }\n",
        "    self._reverse_label_map = reverse_label_map\n",
        "\n",
        "    # Admissible classes\n",
        "    classes = []\n",
        "\n",
        "    for l in use_labels:\n",
        "      classes.append(reverse_label_map[l])\n",
        "\n",
        "    # Map: class --> Images (list)\n",
        "    truncated_dataset = {}\n",
        "\n",
        "    dataset_summary = []\n",
        "    processed_file = \"processed_caltech5.pkl\"\n",
        "\n",
        "    if os.path.exists(processed_file):\n",
        "      print(\"Found processed dataset file. Loading.\")\n",
        "      with open(processed_file, 'rb') as f:\n",
        "        # Unpickle the dictionary\n",
        "        truncated_dataset = pickle.load(f)\n",
        "    else:\n",
        "      print(\"Creating processed dataset...\")\n",
        "      for idx in tqdm(range(0, len(dataloader))):\n",
        "        img, label = dataloader[idx]\n",
        "        if label in classes:\n",
        "          if label not in truncated_dataset:\n",
        "            truncated_dataset[label] = []\n",
        "          truncated_dataset[label].append(img)\n",
        "\n",
        "      with open(processed_file, 'wb') as f:\n",
        "        # Pickle the dictionary\n",
        "        pickle.dump(truncated_dataset, f)\n",
        "        print(\"Saved processed dataset.\")\n",
        "\n",
        "    for label_idx in truncated_dataset.keys():\n",
        "       dataset_summary.append([labels_map[label_idx], len(truncated_dataset[label_idx])])\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(tabulate(dataset_summary, headers=[\"Label\", \"Num. Examples\"]))\n",
        "\n",
        "    # Split for train and test\n",
        "    self._train_split, self._test_split = self._split_dataset(truncated_dataset, split_ratio)\n",
        "    split_summary = []\n",
        "    split_summary.append([\"Train\", len(self._train_split)])\n",
        "    split_summary.append([\"Test\", len(self._test_split)])\n",
        "    print(\"\\n\")\n",
        "    print(tabulate(split_summary, headers=[\"Split Summary\", \"Num. Examples\"]))\n",
        "\n",
        "  def idx2Label(self, index):\n",
        "    return self._labels_map[index]\n",
        "\n",
        "  def _split_dataset(self, dataset, split_ratio):\n",
        "    train_split = {}\n",
        "    test_split = {}\n",
        "\n",
        "    train_examples = 0\n",
        "    test_examples = 0\n",
        "    for label_idx in dataset:\n",
        "      split_index = int(len(dataset[label_idx]) * split_ratio)\n",
        "      train_data = dataset[label_idx][0:split_index]\n",
        "      test_data = dataset[label_idx][split_index:]\n",
        "      train_split[label_idx] = train_data\n",
        "      test_split[label_idx] = test_data\n",
        "\n",
        "      train_examples += len(train_data)\n",
        "      test_examples += len(test_data)\n",
        "\n",
        "    train_split_merged = self._shuffled_dict_to_list(train_split)\n",
        "    test_split_merged = self._shuffled_dict_to_list(test_split)\n",
        "\n",
        "    assert len(train_split_merged) == train_examples\n",
        "    assert len(test_split_merged) == test_examples\n",
        "\n",
        "    return train_split_merged, test_split_merged\n",
        "\n",
        "  def _shuffled_dict_to_list(self, dataset):\n",
        "    merged_list = []\n",
        "    for key in dataset.keys():\n",
        "      for img in dataset[key]:\n",
        "        merged_list.append((img, key))\n",
        "    random.shuffle(merged_list)\n",
        "    return merged_list\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"Split: %s\\nNum Examples: %d\"%(self._split_type, self.__len__())\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    if self._split_type == \"train\":\n",
        "      _split = self._train_split[idx]\n",
        "    else:\n",
        "      _split = self._test_split[idx]\n",
        "\n",
        "    img, class_idx = _split\n",
        "\n",
        "    # Convert Grayscale to RGB\n",
        "    if len(img.size) == 2:\n",
        "      img = img.convert(\"RGB\")\n",
        "\n",
        "    # Apply padding and reize\n",
        "    img = self._base_img_transforms(img)\n",
        "\n",
        "    # Other transforms\n",
        "    if self._img_transform is not None:\n",
        "      img = self._img_transform(img)\n",
        "    return np.moveaxis(np.array(img), -1, 0), class_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    if self._split_type == \"train\":\n",
        "      return len(self._train_split)\n",
        "    else:\n",
        "      return len(self._test_split)\n"
      ],
      "metadata": {
        "id": "avGIu9z1A0LI"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augumentation to apply to image\n",
        "\n",
        "transforms = v2.Compose([\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.5),\n",
        "    v2.RandomPerspective(distortion_scale=0.4, p=.5),\n",
        "    v2.RandomInvert(p=0.5),\n",
        "])\n",
        "\n",
        "training_data = Caltech5(split_ratio=0.8, split=\"test\", transform=transforms)\n",
        "''\n",
        "\n",
        "dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "itr = iter(dataloader)\n",
        "\n",
        "for idx, (train_features, train_labels) in enumerate(itr):\n",
        "  print(idx +1, train_features.shape)\n",
        "  #print(train_labels)\n",
        "  #break\n",
        "\n",
        "\"\"\"\n",
        "random_idxs = np.random.randint(0, len(training_data), size=10)\n",
        "for idx in random_idxs:\n",
        "  img, _class = training_data[idx]\n",
        "  label = training_data.idx2Label(_class)\n",
        "  plt.title(label)\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "6lZpcGDXPqbn",
        "outputId": "48a54d99-5483-480f-8773-ccf1241fd8af"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Found processed dataset file. Loading.\n",
            "\n",
            "\n",
            "Label         Num. Examples\n",
            "----------  ---------------\n",
            "motorbikes              798\n",
            "accordion                55\n",
            "airplanes               800\n",
            "brain                    98\n",
            "butterfly                91\n",
            "\n",
            "\n",
            "Split Summary      Num. Examples\n",
            "---------------  ---------------\n",
            "Train                       1472\n",
            "Test                         370\n",
            "1 torch.Size([32, 3, 300, 300])\n",
            "2 torch.Size([32, 3, 300, 300])\n",
            "3 torch.Size([32, 3, 300, 300])\n",
            "4 torch.Size([32, 3, 300, 300])\n",
            "5 torch.Size([32, 3, 300, 300])\n",
            "6 torch.Size([32, 3, 300, 300])\n",
            "7 torch.Size([32, 3, 300, 300])\n",
            "8 torch.Size([32, 3, 300, 300])\n",
            "9 torch.Size([32, 3, 300, 300])\n",
            "10 torch.Size([32, 3, 300, 300])\n",
            "11 torch.Size([32, 3, 300, 300])\n",
            "12 torch.Size([18, 3, 300, 300])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nrandom_idxs = np.random.randint(0, len(training_data), size=10)\\nfor idx in random_idxs:\\n  img, _class = training_data[idx]\\n  label = training_data.idx2Label(_class)\\n  plt.title(label)\\n  plt.imshow(img)\\n  plt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--tGYajN3iaY",
        "outputId": "e902080d-32fc-4ca8-e83c-2be8b5264f3d"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "      RandomHorizontalFlip(p=0.5)\n",
              "      RandomVerticalFlip(p=0.5)\n",
              "      RandomPerspective(p=0.5, distortion_scale=0.4, interpolation=InterpolationMode.BILINEAR, fill=0)\n",
              "      RandomInvert(p=0.5)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDUnmXGX1iIw",
        "outputId": "d16d6a5d-68c2-470e-e2c9-2a22fd81d695"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=413x199>, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}